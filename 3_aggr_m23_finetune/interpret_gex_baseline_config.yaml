run:
  project_name: finetune_aggr_multiome
  run_name: interpret_training_from_null_m23_L6-IT_Astro_no_chr_split_binary_atac
  use_wandb: false
type: region
stage: predict
assembly: mm10
eval_tss: true
log_image: true
model:
  _target_: get_model.model.model.GETRegionFinetune
  cfg:
    num_regions: 900
    num_motif: 283
    embed_dim: 768
    num_layers: 12
    num_heads: 12
    dropout: 0.1
    output_dim: 2
    flash_attn: false
    pool_method: mean
    region_embed:
      num_regions: ${model.cfg.num_regions}
      num_features: ${model.cfg.num_motif}
      embed_dim: ${model.cfg.embed_dim}
    encoder:
      num_heads: ${model.cfg.num_heads}
      embed_dim: ${model.cfg.embed_dim}
      num_layers: ${model.cfg.num_layers}
      drop_path_rate: ${model.cfg.dropout}
      drop_rate: 0
      attn_drop_rate: 0
      use_mean_pooling: false
      flash_attn: ${model.cfg.flash_attn}
    head_exp:
      embed_dim: ${model.cfg.embed_dim}
      output_dim: ${model.cfg.output_dim}
      use_atac: false
    mask_token:
      embed_dim: ${model.cfg.embed_dim}
      std: 0.02
    loss:
      components:
        exp:
          _target_: torch.nn.PoissonNLLLoss
          reduction: mean
          log_input: false
      weights:
        exp: 1.0
    metrics:
      components:
        exp:
        - pearson
        - spearman
        - r2
machine:
  codebase: /gpfs/home/asun/miniforge3/envs/get/lib/python3.12/site-packages/get_model/
  data_path: /gpfs/home/asun/jin_lab/get/raw_data/
  output_dir: /gpfs/home/asun/jin_lab/get/3_aggr_m23_finetune/output
  num_devices: 1
  num_workers: 2
  batch_size: 8
  fasta_path: ???
dataset:
  zarr_path: /gpfs/home/asun/jin_lab/get/3_aggr_m23_finetune/aggr_multiome_m23.zarr
  celltypes: Astro,Endo,L2/3 IT,L5 ET,L5 IT,L5/6 NP,L6 CT,L6 IT,L6b,Lamp5,Meis2,Micro-PVM,OPC,Oligo,Peri,Pvalb,Sst,VLMC,Vip
  transform: null
  quantitative_atac: false
  sampling_step: 100
  num_region_per_sample: 200
  leave_out_chromosomes: null
  leave_out_celltypes: L6 IT,Astro
  mask_ratio: 0.0
training:
  save_ckpt_freq: 1
  epochs: 10
  warmup_epochs: 1
  accumulate_grad_batches: 1
  clip_grad: null
  use_fp16: false
  log_every_n_steps: 25
  val_check_interval: 0.5
  add_lr_monitor: false
  save_every_n_epochs: null
optimizer:
  lr: 0.0001
  min_lr: 0.0001
  weight_decay: 0.05
  opt: adamw
  opt_eps: 1.0e-08
  opt_betas:
  - 0.9
  - 0.999
finetune:
  resume_ckpt: null
  pretrain_checkpoint: false
  checkpoint: /gpfs/home/asun/jin_lab/get/3_aggr_m23_finetune/checkpoint-best.pth
  strict: true
  model_key: model
  use_lora: false
  lora_checkpoint: null
  rename_config:
    blocks.: encoder.blocks.
    fc_norm.: encoder.norm.
    encoder.head.: head_mask.
    encoder.region_embed: region_embed
    region_embed.proj.: region_embed.embed.
    encoder.cls_token: cls_token
    head.: head_exp.head.
  layers_with_lora:
  - region_embed
  - encoder
  patterns_to_freeze: []
  patterns_to_drop: []
  additional_checkpoints: []
task:
  test_mode: interpret
  gene_list: null
  layer_names: []
  mutations: null
